---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

#Loading Nessesary libraries for sentiment Analysis
```{r}
library(sentimentr)
library(qdap)
library(jsonlite)
library(dplyr)
library(ggplot2)
library(tm)
library(tidyverse)
library(SentimentAnalysis)
library(quanteda)
library(xlsx)
```

#Creating API varible for "Article Search API FORMAT"
```{r}
NYTIMES_KEY="a0HA3uBISDkGyvUGR3FeoAGybtDVPPM5"
```

#creating the query nessesary to extract wanted information

#Professor Johnson still has to give me the list of words nessesary for query 

#Doesn't seem like we can do "and" "or" for the specific query, something that academic search complete and pro quest are able to do

#Just do it for one month 2012/01/01 - 2012/02/01

#The default connector for values in parentheses is OR. 
```{r}
# Let's set some parameters
#term <- "immigrant+immigration" # Need to use + to string together separate words however it is concatinating
begin_date <- "20181130"  
end_date <- "20200101"
oldest <- "oldest"
newest_var<-"newest"
article <- "article"
news <- "News"
#glocations <- "U.S"
```

#Searching for the terms that we query above and essentially creating the query object in order to use it for the API

#the https needs to be underlined

#immigrant OR migrant OR migration OR refugee OR asylum OR undocumented"

#it needs to be fq not q


##Search works with parenthesis##
```{r}


baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?fq=lead_paragraph:(migrant OR immigration OR immigrant OR migration OR refugee OR alien OR undocumented OR asylum) AND type_of_material:news ","&sort=",oldest,
                  "&begin_date=",begin_date,"&end_date=",end_date,
                  "&facet_filter=true&api-key=",NYTIMES_KEY, sep=" ")
```


#Returning a Json object and calculating the max pages from the query, there is a max of 10 objects (newspaper articles) per page
```{r}
initialQuery <- fromJSON(baseurl)
maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1) 
```

#from the maxpages we are pasting the baseurl and retriving the information and putting it in a dataframe

#Putting the system to sleep in order to 'trick' the computer for the amount of requests made per minute

#query from JAN 2012 - 2019 JAN took >24 minutes to run with sleep 30

#it doesn't like it when using %>% data.frame() in the end of the third line
```{r}
#require(reshape2)
pages_extract <- list()
for(i in 0:maxPages){
  nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame() 
  message("Retrieving page ", i)
  pages_extract[[i+1]] <- nytSearch 
  Sys.sleep(30) #change to 5 to test on smaller datasets, 30 for actual execution
}
```


```{r}
#allNYTSearch_try_1982_06_27 <-rbind_pages(pages_extract)


```


```{r}
#allNYTSearch_try_1984_10_26 <-rbind_pages(pages_extract)
```



```{r}
#allNYTSearch_try_1985_06_02 <- rbind_pages(pages_extract)
```


```{r}
#allNYTSearch_try1987_11_21 <- rbind_pages(pages_extract)
```


```{r}
#allNYTSearch_try1989_11_29 <- rbind_pages(pages_extract)
```


```{r}
#allNYTSearch_try1990_11_04 <- rbind_pages(pages_extract)
```


```{r}
#allNYTSearch_try1992_11_29 <- rbind_pages(pages_extract)
```


```{r}
#allNYTSearch_try1994_11_27 <- rbind_pages(pages_extract)
```

```{r}
#allNYTSearch_try1996_02_26 <- rbind_pages(pages_extract)
```


```{r}
#allNYTSearch_try1998_01_14 <- rbind_pages(pages_extract)
```


```{r}
#allNYTSearch_try1999_12_27 <- rbind_pages(pages_extract)
```


```{r}
#allNYTSearch_try2001_10_10 <- rbind_pages(pages_extract)
```


```{r}
#allNYTSearch_try2003_01_01 <- rbind_pages(pages_extract)
```


```{r}
#allNYTSearch_try2004_11_21 <- rbind_pages(pages_extract)
```



```{r}
#allNYTSearch_try2006_01_01 <- rbind_pages(pages_extract)
```


```{r}
#allNYTSearch_try2006_05_04 <- rbind_pages(pages_extract)
```


```{r}
#allNYTSearch_try2006_06_18 <- rbind_pages(pages_extract)
```



```{r}
#allNYTSearch_try2007_02_01 <- rbind_pages(pages_extract)
```



```{r}
#allNYTSearch_try2007_06_27 <- rbind_pages(pages_extract)
```




```{r}
#allNYTSearch_try2008_04_25 <- rbind_pages(pages_extract)
```

```{r}
#allNYTSearch_try2008_10_14 <- rbind_pages(pages_extract)
```


```{r}
#allNYTSearch_try2009_11_23 <- rbind_pages(pages_extract)
```

```{r}
#allNYTSearch_try2010_04_30 <- rbind_pages(pages_extract)
```


```{r}
#allNYTSearch_try2011_12_01 <- rbind_pages(pages_extract)
```


```{r}
#allNYTSearch_try2013_06_01 <- rbind_pages(pages_extract)
```

```{r}
#allNYTSearch_try2014_12_10 <- rbind_pages(pages_extract)
```



```{r}
#allNYTSearch_try2015_11_11 <- rbind_pages(pages_extract)
```


```{r}
#allNYTSearch_try2016_06_16 <- rbind_pages(pages_extract)
```



```{r}
#allNYTSearch_try2017_01_26 <- rbind_pages(pages_extract)
```

```{r}
#allNYTSearch_try2017_09_19 <- rbind_pages(pages_extract)
```


```{r}
#allNYTSearch_try2018_11_30 <- rbind_pages(pages_extract)
```


```{r}
#allNYTSearch_try2019_12_31 <- rbind_pages(pages_extract)
```



#binding dataframes
```{r}
result_all_dfs<-do.call("rbind", list(allNYTSearch_try_1982_06_27,allNYTSearch_try_1984_10_26,allNYTSearch_try_1985_06_02,allNYTSearch_try1987_11_21,allNYTSearch_try1989_11_29,allNYTSearch_try1990_11_04,allNYTSearch_try1992_11_29,allNYTSearch_try1994_11_27,allNYTSearch_try1996_02_26,allNYTSearch_try1998_01_14, allNYTSearch_try1999_12_27,allNYTSearch_try2001_10_10, allNYTSearch_try2003_01_01, allNYTSearch_try2004_11_21, allNYTSearch_try2006_01_01, allNYTSearch_try2006_05_04, allNYTSearch_try2006_06_18, allNYTSearch_try2007_02_01, allNYTSearch_try2007_06_27, allNYTSearch_try2008_04_25, allNYTSearch_try2008_10_14, allNYTSearch_try2009_11_23, allNYTSearch_try2010_04_30, allNYTSearch_try2011_12_01, allNYTSearch_try2013_06_01, allNYTSearch_try2014_12_10, allNYTSearch_try2015_11_11, allNYTSearch_try2016_06_16, allNYTSearch_try2017_01_26, allNYTSearch_try2017_09_19, allNYTSearch_try2018_11_30, allNYTSearch_try2019_12_31))
```

#removing duplicate ids, since we start the API from the last day that it was received
```{r}
result_all_dfs_no_duplicate<-result_all_dfs[!duplicated(result_all_dfs$response.docs._id), ]
```



#extracting the years that were published in this analysis
```{r}
allNYTSearch1980_year$year_published<-substr(allNYTSearch1980_year$response.docs.pub_date, 1, 4)
```



#saving data from 1980 to 2020 for sentiment analysis
```{r}
allNYTSearchtosave <- result_all_dfs_no_duplicate
allNYTSearchtosave$response.docs.multimedia <- NULL
allNYTSearchtosave$response.docs.keywords <- NULL
allNYTSearchtosave$response.docs.byline.person<- NULL
#write.xlsx(allNYTSearch, 'allNYTSearch2012-2019-1459hits.xlsx')
write.table(allNYTSearchtosave, "allNYTSearch1980to2020_no_duplicates.txt", sep="\t")
```



