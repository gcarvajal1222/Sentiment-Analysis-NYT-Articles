{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nytimesarticle import articleAPI\n",
    "#api = articleAPI('a0HA3uBISDkGyvUGR3FeoAGybtDVPPM5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#articles = api.search( q = 'immigration', \n",
    "#     begin_date = 20120101, end_date =20120201 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create TIDF on an example call from R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/latent-semantic-analysis-sentiment-classification-with-python-5f657346f6a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "import nltk\n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models.wrappers import FastText \n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "\n",
    "from PyDictionary import PyDictionary # utilize this library in order to do webscrapping\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "import nltk \n",
    "import pandas as pd\n",
    "from pandas import *\n",
    "import pandasql\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "from builtins import input\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "\n",
    "from statistics import mode, StatisticsError\n",
    "\n",
    "import functools\n",
    "\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "global cases\n",
    "\n",
    "from pandas import ExcelWriter\n",
    "import xlsxwriter\n",
    "\n",
    "import xlwt\n",
    "from xlwt.Workbook import *\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from statistics import *\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Sentiment-Analysis-NYT-Articlesdir\\\\allNYTSearch1980to2020.txt', sep = \"\\t\", encoding = 'iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['status', 'copyright', 'response.docs.abstract',\n",
       "       'response.docs.web_url', 'response.docs.snippet',\n",
       "       'response.docs.lead_paragraph', 'response.docs.print_section',\n",
       "       'response.docs.print_page', 'response.docs.source',\n",
       "       'response.docs.pub_date', 'response.docs.document_type',\n",
       "       'response.docs.news_desk', 'response.docs.section_name',\n",
       "       'response.docs.type_of_material', 'response.docs._id',\n",
       "       'response.docs.word_count', 'response.docs.uri',\n",
       "       'response.docs.headline.main', 'response.docs.headline.kicker',\n",
       "       'response.docs.headline.content_kicker',\n",
       "       'response.docs.headline.print_headline', 'response.docs.headline.name',\n",
       "       'response.docs.headline.seo', 'response.docs.headline.sub',\n",
       "       'response.docs.byline.original', 'response.docs.byline.organization',\n",
       "       'response.meta.hits', 'response.meta.offset', 'response.meta.time',\n",
       "       'response.docs.subsection_name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess evertyhing in python and not R: migrating code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    #tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    #tokens = tokenizer.tokenize(rem_num)\n",
    "    tokens = nltk.tokenize.word_tokenize(rem_num) \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    #stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    #lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "prohibitedWords = []\n",
    "\n",
    "def preprocess_regex(row):\n",
    "    row=str(row)\n",
    "    row = row.lower()\n",
    "    row=row.replace('-',\" \") \n",
    "    row=row.replace(',',\" \")\n",
    "    row=row.replace('.',\" \")\n",
    "    row=row.replace('+',\" \")\n",
    "    row=row.replace('\"',\" \")\n",
    "    row=row.replace(\"''\",\"\")\n",
    "    big_regex = re.compile(r'\\b%s\\b' % r'\\b|\\b'.join(map(re.escape, prohibitedWords)))  #this would replace 'random' in 'random words' but not in 'pseudorandom words'\n",
    "    cleantext = re.sub(big_regex, '', row)\n",
    "    rem_num = re.sub('[0-9]+', '', cleantext)\n",
    "    #tokenizer = RegexpTokenizer(r'\\w+')  #this yields lower counts of words\n",
    "    tokens = nltk.tokenize.word_tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')] #if it appears more than two times get rid off the stopwords, change this\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleanText']=df['response.docs.lead_paragraph'].map(lambda s:preprocess(s)) \n",
    "\n",
    "Tokenized_lead_paragraphs = df.apply(lambda row: nltk.word_tokenize(row['cleanText']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing_words_in_product_title(df, col):    \n",
    "    pt_clean = df[col].str.cat(sep=' ')\n",
    "    words = nltk.tokenize.word_tokenize(pt_clean)\n",
    "    word_dist = nltk.FreqDist(words)\n",
    "    top_N = len(words)\n",
    "\n",
    "    #creating a dataframe of these words plus id so later we can merge.\n",
    "    C_UW_F= pd.DataFrame(word_dist.most_common(top_N),  #C_UW_F = category unique word final\n",
    "                    columns=['Word', 'Frequency'])\n",
    "    C_UW_F[\"id\"] = C_UW_F.index + 1\n",
    "    return (C_UW_F)\n",
    "#C_UW_F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_words_corpus=tokenizing_words_in_product_title(df, 'cleanText')\n",
    "\n",
    "#U_words_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_query = U_words_corpus['Word'] #words and its meanings\n",
    "\n",
    "meanings_for_words_in_product_title = [] \n",
    "for word in list_query: # Your word list\n",
    "    ss = wordnet.synsets(word)\n",
    "    if ss:\n",
    "        definition = ss[0].definition() \n",
    "        meanings_for_words_in_product_title.append(definition)\n",
    "    else:\n",
    "        word = None\n",
    "        meanings_for_words_in_product_title.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meanings_for_words_in_product_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8709"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unqiue_words = pd.DataFrame({'word':list_query, 'tag':meanings_for_words_in_product_title})\n",
    "\n",
    "df_unqiue_words['counts_of_words'] = U_words_corpus['Frequency']\n",
    "\n",
    "len(df_unqiue_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Senitment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'michael wright caroline rand herron last week pedro rodriguez must wondered whether boat took wrong turn last april cuban refugee boatlift shortly arrived united states trundled federal penitentiary leavenworth kan'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleanText'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer_scores(sentence):\n",
    "    scores = analyser.polarity_scores(sentence)\n",
    "    #score = (\"{:-<40} {}\".format(sentence, str(string_words)))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text=list(df['cleanText'])\n",
    "\n",
    "sentiment_scores = []\n",
    "for i in range(len(clean_text)):\n",
    "    sentiment_scores.append(sentiment_analyzer_scores(clean_text[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_scores'] = sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21629"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pdgac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\pdgac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "\n",
    "\n",
    "def get_word_sentiment(text):\n",
    "    neg_word_list = []\n",
    "    pos_word_list = []\n",
    "    neu_word_list = []\n",
    "    \n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    \n",
    "    for word in tokenized_text:\n",
    "        if (analyser.polarity_scores(word)['compound']) >= 0.1:\n",
    "            pos_word_list.append(word)\n",
    "        elif (analyser.polarity_scores(word)['compound']) <= -0.1:\n",
    "            neg_word_list.append(word)\n",
    "        else:\n",
    "            neu_word_list.append(word)\n",
    "    return {'Positive':pos_word_list,'Neutral': neu_word_list,'Negative':neg_word_list}\n",
    "    #return pos_word_list,neu_word_list,neg_word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists_sentiment_P_N_N = []\n",
    "for i in range(len(df)):\n",
    "    lists_sentiment_P_N_N.append(get_word_sentiment(clean_text[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1459"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lists_sentiment_P_N_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lists_sentiment_P_N_N' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4f543c04f89b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlists_sentiment_P_N_N\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lists_sentiment_P_N_N' is not defined"
     ]
    }
   ],
   "source": [
    "lists_sentiment_P_N_N[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF (Term Frequency) measures the frequency of a word in a document.\n",
    "\n",
    "TF = (Number of time the word occurs in the text) / (Total number of words in text)\n",
    "\n",
    "IDF (Inverse Document Frequency) measures the rank of the specific word for its relevancy within the text. Stop words which contain unnecessary information such as “a”, “into” and “and” carry less importance in spite of their occurrence.\n",
    "\n",
    "IDF = (Total number of documents / Number of documents with word t in it)\n",
    "\n",
    "Thus, the TF-IDF is the product of TF and IDF:\n",
    "\n",
    "TF-IDF = TF * IDF\n",
    "\n",
    "https://iyzico.engineering/how-to-calculate-tf-idf-term-frequency-inverse-document-frequency-from-the-beatles-biography-in-c4c3cd968296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer( min_df=3, max_df=0.5, ngram_range=(1,2))\n",
    "sf = cvec.fit_transform(df['cleanText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "transformed_weights = transformer.fit_transform(sf)\n",
    "weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': cvec.get_feature_names(), 'weight': weights})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>migrants</td>\n",
       "      <td>0.021299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>border</td>\n",
       "      <td>0.018927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3143</th>\n",
       "      <td>trump</td>\n",
       "      <td>0.018896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3209</th>\n",
       "      <td>united</td>\n",
       "      <td>0.018171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>president</td>\n",
       "      <td>0.017406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>states</td>\n",
       "      <td>0.016919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3211</th>\n",
       "      <td>united states</td>\n",
       "      <td>0.016074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>immigration</td>\n",
       "      <td>0.015551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>mexico</td>\n",
       "      <td>0.015008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>new</td>\n",
       "      <td>0.014969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>europe</td>\n",
       "      <td>0.014357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>european</td>\n",
       "      <td>0.014115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3415</th>\n",
       "      <td>year</td>\n",
       "      <td>0.013857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>asylum</td>\n",
       "      <td>0.012693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085</th>\n",
       "      <td>one</td>\n",
       "      <td>0.012605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289</th>\n",
       "      <td>president trump</td>\n",
       "      <td>0.012249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>germany</td>\n",
       "      <td>0.012204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>country</td>\n",
       "      <td>0.011989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>refugees</td>\n",
       "      <td>0.011928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>last</td>\n",
       "      <td>0.011448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2586</th>\n",
       "      <td>said</td>\n",
       "      <td>0.011199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186</th>\n",
       "      <td>people</td>\n",
       "      <td>0.011034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3201</th>\n",
       "      <td>union</td>\n",
       "      <td>0.011008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3304</th>\n",
       "      <td>washington</td>\n",
       "      <td>0.010909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>get</td>\n",
       "      <td>0.010448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>administration</td>\n",
       "      <td>0.010189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3399</th>\n",
       "      <td>would</td>\n",
       "      <td>0.010079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>children</td>\n",
       "      <td>0.009987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>european union</td>\n",
       "      <td>0.009942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>immigrants</td>\n",
       "      <td>0.009908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>crisis</td>\n",
       "      <td>0.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3044</th>\n",
       "      <td>thousands</td>\n",
       "      <td>0.009483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2735</th>\n",
       "      <td>sign</td>\n",
       "      <td>0.009465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>berlin</td>\n",
       "      <td>0.009328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>week</td>\n",
       "      <td>0.009164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>editor</td>\n",
       "      <td>0.009064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>city</td>\n",
       "      <td>0.008949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1875</th>\n",
       "      <td>migration</td>\n",
       "      <td>0.008839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3178</th>\n",
       "      <td>two</td>\n",
       "      <td>0.008821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1840</th>\n",
       "      <td>migrant</td>\n",
       "      <td>0.008687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2528</th>\n",
       "      <td>right</td>\n",
       "      <td>0.008667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3420</th>\n",
       "      <td>years</td>\n",
       "      <td>0.008375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3284</th>\n",
       "      <td>want</td>\n",
       "      <td>0.008307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>american</td>\n",
       "      <td>0.008281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>here</td>\n",
       "      <td>0.008209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>central</td>\n",
       "      <td>0.008170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>government</td>\n",
       "      <td>0.008026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>email</td>\n",
       "      <td>0.007967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218</th>\n",
       "      <td>up</td>\n",
       "      <td>0.007960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>friday</td>\n",
       "      <td>0.007830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 term    weight\n",
       "1847         migrants  0.021299\n",
       "367            border  0.018927\n",
       "3143            trump  0.018896\n",
       "3209           united  0.018171\n",
       "2285        president  0.017406\n",
       "2850           states  0.016919\n",
       "3211    united states  0.016074\n",
       "1446      immigration  0.015551\n",
       "1823           mexico  0.015008\n",
       "1993              new  0.014969\n",
       "978            europe  0.014357\n",
       "985          european  0.014115\n",
       "3415             year  0.013857\n",
       "231            asylum  0.012693\n",
       "2085              one  0.012605\n",
       "2289  president trump  0.012249\n",
       "1232          germany  0.012204\n",
       "649           country  0.011989\n",
       "2438         refugees  0.011928\n",
       "1618             last  0.011448\n",
       "2586             said  0.011199\n",
       "2186           people  0.011034\n",
       "3201            union  0.011008\n",
       "3304       washington  0.010909\n",
       "1234              get  0.010448\n",
       "42     administration  0.010189\n",
       "3399            would  0.010079\n",
       "524          children  0.009987\n",
       "990    european union  0.009942\n",
       "1441       immigrants  0.009908\n",
       "680            crisis  0.009500\n",
       "3044        thousands  0.009483\n",
       "2735             sign  0.009465\n",
       "327            berlin  0.009328\n",
       "3330             week  0.009164\n",
       "913            editor  0.009064\n",
       "545              city  0.008949\n",
       "1875        migration  0.008839\n",
       "3178              two  0.008821\n",
       "1840          migrant  0.008687\n",
       "2528            right  0.008667\n",
       "3420            years  0.008375\n",
       "3284             want  0.008307\n",
       "130          american  0.008281\n",
       "1347             here  0.008209\n",
       "494           central  0.008170\n",
       "1260       government  0.008026\n",
       "935             email  0.007967\n",
       "3218               up  0.007960\n",
       "1198           friday  0.007830"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_df.sort_values(by='weight', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('caravan', 0.9978567361831665), ('mexico', 0.9973210692405701), ('crossing', 0.9973007440567017), ('american', 0.9972114562988281), ('southern', 0.9970765113830566), ('america', 0.9969837665557861), ('surge', 0.9969432353973389), ('northern', 0.9967454075813293), ('guatemala', 0.9966573715209961), ('san', 0.9965139031410217)]\n"
     ]
    }
   ],
   "source": [
    "model_by_category = gensim.models.Word2Vec(Tokenized_lead_paragraphs, min_count = 10,  \n",
    "                               window = 5, sg = 1) #CBOW method, the idea is that given a context, we want to know which word is most likely to appear in it\n",
    "\n",
    "vocabulary = model_by_category.wv.vocab\n",
    "#print(vocabulary)\n",
    "\n",
    "\n",
    "sim_words = model_by_category.wv.most_similar('central', topn = 10)\n",
    "print(sim_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_most_similar_words_lead_P = FastText(Tokenized_lead_paragraphs, size=100, window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('immigration', 0.9999980926513672),\n",
       " ('generation', 0.9999960660934448),\n",
       " ('nation', 0.9999956488609314),\n",
       " ('separation', 0.9999954700469971),\n",
       " ('deportation', 0.9999953508377075),\n",
       " ('coalition', 0.9999949932098389),\n",
       " ('detention', 0.9999946355819702),\n",
       " ('population', 0.9999945163726807),\n",
       " ('destination', 0.9999943971633911),\n",
       " ('station', 0.999994158744812),\n",
       " ('action', 0.9999940395355225),\n",
       " ('attention', 0.9999939203262329),\n",
       " ('edition', 0.9999937415122986),\n",
       " ('nomination', 0.9999936819076538),\n",
       " ('situation', 0.9999935626983643),\n",
       " ('nations', 0.9999933838844299),\n",
       " ('deportations', 0.9999932646751404),\n",
       " ('investigation', 0.9999927878379822),\n",
       " ('legislation', 0.9999927282333374),\n",
       " ('nationwide', 0.9999927282333374),\n",
       " ('question', 0.9999924302101135),\n",
       " ('election', 0.9999923706054688),\n",
       " ('administration', 0.9999923706054688),\n",
       " ('conditions', 0.9999923706054688),\n",
       " ('national', 0.9999922513961792),\n",
       " ('organization', 0.9999922513961792),\n",
       " ('opposition', 0.9999921321868896),\n",
       " ('protection', 0.9999918341636658),\n",
       " ('solution', 0.9999915361404419),\n",
       " ('construction', 0.9999913573265076),\n",
       " ('nationalist', 0.9999912977218628),\n",
       " ('immigrants', 0.999991238117218),\n",
       " ('international', 0.9999911189079285),\n",
       " ('additional', 0.999990701675415),\n",
       " ('persecution', 0.9999906420707703),\n",
       " ('nation\\x92s', 0.9999906420707703),\n",
       " ('restrictions', 0.9999905824661255),\n",
       " ('immigrant', 0.9999905228614807),\n",
       " ('traditional', 0.9999904036521912),\n",
       " ('migrants', 0.9999902248382568),\n",
       " ('questions', 0.9999901056289673),\n",
       " ('administration\\x92s', 0.9999898672103882),\n",
       " ('elections', 0.9999898076057434),\n",
       " ('questioned', 0.9999897480010986),\n",
       " ('centers', 0.9999897480010986),\n",
       " ('million', 0.9999897480010986),\n",
       " ('center', 0.9999896287918091),\n",
       " ('residents', 0.9999896287918091),\n",
       " ('parents', 0.9999896287918091),\n",
       " ('government', 0.9999892711639404)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_most_similar_words_lead_P.wv.most_similar(\"migration\", topn = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
