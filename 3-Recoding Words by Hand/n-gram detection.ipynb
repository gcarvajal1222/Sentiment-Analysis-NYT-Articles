{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wqlb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\wqlb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\wqlb\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "import nltk\n",
    "#from gensim.models import Word2Vec \n",
    "#from gensim.models.wrappers import FastText \n",
    "\n",
    "#from gensim.models import FastText\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "import nltk \n",
    "import pandas as pd\n",
    "from pandas import *\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "from builtins import input\n",
    "from statistics import mean\n",
    "from nltk import  pos_tag_sents\n",
    "\n",
    "\n",
    "from statistics import mode, StatisticsError\n",
    "\n",
    "import functools\n",
    "\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "global cases\n",
    "\n",
    "from pandas import ExcelWriter\n",
    "import xlsxwriter\n",
    "\n",
    "import xlwt\n",
    "from xlwt.Workbook import *\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from statistics import *\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import seaborn as sns\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first_run = pd.read_excel(\"NYT_data_1980_to_2020_V2.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using TF-IDF to identify problematic n-grams and keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proper Noun Understanding and word relocation for the first 200 words in negative, 200 words in positive 300 words in neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF (Term Frequency) measures the frequency of a word in a document.\n",
    "\n",
    "TF = (Number of time the word occurs in the text) / (Total number of words in text)\n",
    "\n",
    "IDF (Inverse Document Frequency) measures the rank of the specific word for its relevancy within the text. Stop words which contain unnecessary information such as “a”, “into” and “and” carry less importance in spite of their occurrence.\n",
    "\n",
    "IDF = (Total number of documents / Number of documents with word t in it)\n",
    "\n",
    "Thus, the TF-IDF is the product of TF and IDF:\n",
    "\n",
    "TF-IDF = TF * IDF\n",
    "\n",
    "https://iyzico.engineering/how-to-calculate-tf-idf-term-frequency-inverse-document-frequency-from-the-beatles-biography-in-c4c3cd968296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer( min_df=2, ngram_range=(2,4), lowercase = False)\n",
    "sf = cvec.fit_transform(df_first_run['cleanText'].values.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "transformed_weights = transformer.fit_transform(sf)\n",
    "weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'Word': cvec.get_feature_names(), 'weight': weights})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_df=weights_df.sort_values(by='weight', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43001</th>\n",
       "      <td>United States</td>\n",
       "      <td>0.011630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42847</th>\n",
       "      <td>United Nations</td>\n",
       "      <td>0.004247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39254</th>\n",
       "      <td>The United</td>\n",
       "      <td>0.001556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53745</th>\n",
       "      <td>asylum United</td>\n",
       "      <td>0.001238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53746</th>\n",
       "      <td>asylum United States</td>\n",
       "      <td>0.001238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42843</th>\n",
       "      <td>United Kingdoms admission Continued</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42878</th>\n",
       "      <td>United Nations also</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42879</th>\n",
       "      <td>United Nations also NATO</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43119</th>\n",
       "      <td>United States Most important</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43156</th>\n",
       "      <td>United States Soviet Union</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3197 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Word    weight\n",
       "43001                        United States  0.011630\n",
       "42847                       United Nations  0.004247\n",
       "39254                           The United  0.001556\n",
       "53745                        asylum United  0.001238\n",
       "53746                 asylum United States  0.001238\n",
       "...                                    ...       ...\n",
       "42843  United Kingdoms admission Continued  0.000001\n",
       "42878                  United Nations also  0.000001\n",
       "42879             United Nations also NATO  0.000001\n",
       "43119         United States Most important  0.000001\n",
       "43156           United States Soviet Union  0.000001\n",
       "\n",
       "[3197 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_df[n_gram_df['Word'].str.contains((r'(?:\\s|^|Ei:|EI:|EI-)'+'United'+r'(?:\\s|$)'), case=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analysis excel TIDF analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams_to_remove=[\"United States\", \"Supreme Court\", \"Homeland Security\", \n",
    "                   \"Social Security\", \"Justice Department\", \"Great Depression\",\n",
    "                  \"United Nations\", \"Statue of liberty\", \"Central Intelligence\", \"Lower East\", \n",
    "                  \"Star Wars\", \"New York\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
